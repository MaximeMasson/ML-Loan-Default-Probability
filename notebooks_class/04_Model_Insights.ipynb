{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b94afe",
   "metadata": {},
   "source": [
    "# 04 - Model Insights\n",
    "\n",
    "This notebook provides insights derived from the trained models. It includes performance analysis, key takeaways, and recommendations for improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Insights from trained models include performance analysis and improvement recommendations.\n",
    "- PCA reduced Gradient Boosting model performance, indicating loss of key features.\n",
    "- Feature engineering and selection improved model performance, with Gradient Boosting as the top performer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _Model_Comparator_class import modelComparator\n",
    "import pandas as pd\n",
    "\n",
    "feature_selec_df = pd.read_parquet('data/feature_selec_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b99b0d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Best Params': {'model__n_estimators': 220, 'model__learning_rate': 0.018936332533954676, 'model__max_depth': 5, 'model__subsample': 0.6565243115041418, 'model__random_state': 42}, 'Recall': 0.9272727272727272, 'Precision': 0.796875, 'Accuracy': 0.8454545454545455, 'F1 Score': 0.8571428571428571, 'AUC ROC': 0.8454545454545455, 'Model': 'gradient_boosting PCA', 'Processing Time (s)': 1625.2171738147736}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Model</th>\n",
       "      <th>Processing Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'model__n_estimators': 220, 'model__learning_...</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>gradient_boosting PCA</td>\n",
       "      <td>1625.217174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Best Params    Recall  Precision  \\\n",
       "0  {'model__n_estimators': 220, 'model__learning_...  0.927273   0.796875   \n",
       "\n",
       "   Accuracy  F1 Score   AUC ROC                  Model  Processing Time (s)  \n",
       "0  0.845455  0.857143  0.845455  gradient_boosting PCA          1625.217174  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_test = ['gradient_boosting PCA']\n",
    "model_comparator_feature_selec = modelComparator(feature_selec_df)\n",
    "model_comparator_feature_selec.compare_models(model_to_test, 10, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee44f13",
   "metadata": {},
   "source": [
    "## 1. Key Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the best model from model_comparator_feature_selec\n",
    "best_model = model_comparator_feature_selec.best_model\n",
    "\n",
    "# Ensure that the test set is not used in any part of the model training or hyperparameter tuning process\n",
    "X_test = model_comparator_feature_selec.X_test\n",
    "y_test = model_comparator_feature_selec.y_test\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, model_comparator_feature_selec.y_pred)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e3463",
   "metadata": {},
   "source": [
    "This ROC curve demonstrates strong model performance, as indicated by the high AUC value of 0.96. The curve stays close to the top-left corner, which suggests that the model achieves a high true positive rate while maintaining a low false positive rate. This is indicative of excellent discriminatory power, meaning the model is effective at distinguishing between the positive and negative classes. The diagonal line represents a random guess (AUC = 0.5), and the model's curve significantly outperforms this baseline, showcasing its robustness and reliability in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities for the test set, selecting only the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions (using a threshold of 0.5)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay(conf_matrix).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adc22c",
   "metadata": {},
   "source": [
    "\n",
    "The results from the confusion matrix and classification report reflect a highly effective model with balanced performance across both classes. The confusion matrix shows that the model accurately identifies most instances, with 1051 true negatives and 1060 true positives. There are relatively few misclassifications, including 49 false positives and 40 false negatives, indicating strong model precision and recall.\n",
    "\n",
    "The classification report confirms this, with both classes achieving precision and recall scores of approximately 0.96. The F1-scores for both classes are also 0.96, highlighting a well-balanced performance where precision and recall are effectively aligned. An overall accuracy of 96% underscores the model's reliability and its capability to generalize well to the dataset.\n",
    "\n",
    "These results suggest that the model is robust and capable of making accurate predictions with minimal misclassification, which is crucial for maintaining high performance in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bba1e",
   "metadata": {},
   "source": [
    "## 2. Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f339f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = ['random_forest + xgboost + gradient_boosting', 'gradient_boosting PCA']\n",
    "model_comparator_feature_selec = modelComparator(feature_selec_df)\n",
    "model_comparator_feature_selec.compare_models(model_to_test, 10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498661f",
   "metadata": {},
   "source": [
    "\n",
    "Applying PCA (Principal Component Analysis) in this case resulted in a notable decline in the performance of the Gradient Boosting model. Initially, without PCA, the model achieved an impressive Accuracy of 0.959545 and an AUC ROC of 0.96. However, after applying PCA, the Accuracy dropped to 0.85 and the AUC ROC to 0.85.\n",
    "\n",
    "This decline indicates that the transformation and dimensionality reduction introduced by PCA did not preserve the necessary information and relationships within the data that were critical for effective classification. While PCA can help reduce noise and computational complexity, it may also eliminate important variance and features that contribute to model performance, leading to suboptimal results. In this instance, the loss of key features likely hindered the model's ability to capture the underlying patterns in the dataset, resulting in reduced predictive capability and overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2452e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249a0b9",
   "metadata": {},
   "source": [
    "The modeling process began with a comprehensive evaluation of several algorithms, yielding the following initial results for various models:\n",
    "\n",
    "Gradient Boosting achieved an AUC ROC score of 0.87, while XGBoost reached an AUC ROC score of 0.86. Other models such as Random Forest and Logistic Regression showed similar performance metrics.\n",
    "After implementing feature engineering, we observed significant improvements across the board. Specifically, Gradient Boosting improved its performance with an Accuracy of 0.94 and an AUC ROC score of 0.94. \n",
    "\n",
    "Focusing on Gradient Boosting, further feature selection yielded an even higher Accuracy of 0.96 and AUC ROC score of 0.96. This improvement confirmed the efficacy of narrowing down the feature set, leading to a model that effectively captured the underlying patterns in the data.\n",
    "\n",
    "Subsequently, PCA was applied in an attempt to enhance dimensionality reduction, but this did not yield positive results, as the Accuracy and AUC ROC scores fell to 0.85 and 0.85, respectively. The application of PCA highlighted the importance of maintaining key features, as the transformation appeared to have removed critical information necessary for optimal classification.\n",
    "\n",
    "In conclusion, the iterative process of feature engineering and selection demonstrated substantial gains in model performance, particularly for Gradient Boosting, which emerged as the top-performing algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
